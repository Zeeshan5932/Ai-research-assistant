# ğŸ§  AI Research Assistant Agent

An **agentic AI research assistant** built with **LangChain**, **FastAPI**, and **Streamlit**. The system can search academic papers, summarize research, and act as an autonomous tool-using agent for students, researchers, and consultants.

---

## ğŸš€ Features

* ğŸ” Search academic papers (Arxiv)
* ğŸ§  Agent-based reasoning (LangChain)
* ğŸ“ Summarize research papers
* ğŸ’¬ Conversational memory
* ğŸŒ FastAPI backend
* ğŸ–¥ï¸ Streamlit frontend (chat UI)
* ğŸ“š Extensible to PDF RAG + citations

---

## ğŸ—ï¸ Project Architecture

```
frontend/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ api.py                # Backend API calls
# ğŸ§  AI Research Assistant

An agentic AI research assistant that searches, summarizes, and reasons over academic papers. Built with FastAPI (backend) and Streamlit (frontend), and designed for easy local development and extension (PDF RAG, citation support, vector search).

---

## ğŸš€ Features

- Search academic papers (ArXiv)
- Agentic reasoning workflows (LangChain)
- Summarize papers and extract key insights
- Conversational memory for multi-turn sessions
- Pluggable vector store for retrieval (FAISS/others)
- Streamlit chat UI and FastAPI backend

---

## ğŸ—ï¸ Repository layout

```
.
â”œâ”€â”€ backend/                # FastAPI backend, agents, services
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ agent/
â”‚       â”œâ”€â”€ research_agent.py
â”‚       â”œâ”€â”€ tools.py
â”‚       â”œâ”€â”€ memory.py
â”‚       â””â”€â”€ prompts.py
â”œâ”€â”€ frontend/               # Streamlit frontend (chat UI)
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ components.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ models/                 # Pydantic schemas, DTOs
â”‚   â””â”€â”€ schemas.py
â””â”€â”€ README.md
```

---

## ğŸ§° Tech stack

- Python 3.10+
- FastAPI (backend)
- Streamlit (frontend)
- LangChain (agent workflows)
- OpenAI or other LLM providers
- FAISS or other vector stores for retrieval

---

## âš™ï¸ Quick setup (local)

Prerequisites: Python 3.10+, git, and an OpenAI API key (or another LLM provider key).

1. Clone the repo

```bash
git clone <repo-url>
cd Ai-research-assistant
```

2. Create and activate a virtual environment (recommended)

Windows (PowerShell):

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

macOS / Linux:

```bash
python -m venv .venv
source .venv/bin/activate
```

3. Install backend dependencies and run the API

```bash
cd backend
pip install -r requirements.txt
uvicorn app:app --reload --port 8000
```

The backend will be available at: http://127.0.0.1:8000

4. Install frontend dependencies and run the Streamlit app

```bash
cd ../frontend
pip install -r requirements.txt
streamlit run app.py
```

The frontend will be available at: http://localhost:8501

---

## ğŸ” Environment variables

Create a `.env` file in the repository root (or set these in your environment):

```
OPENAI_API_KEY=your_openai_key_here
BACKEND_URL=http://127.0.0.1:8000
```

Adjust other variables in `backend/config.py` or `frontend/config.py` as needed.

---

## ğŸ§ª Usage examples

- Query: "Find recent papers on anomaly detection in finance"
- Request a summary: "Summarize this paper and list limitations"

Use the Streamlit UI for interactive querying, or call the FastAPI endpoints directly for integration.

---

## ğŸ”­ Development notes

- The `backend/agent` module contains agent logic and tools.
- Services implementing search, PDF ingestion, and vector operations live in `backend/services`.
- Add vector data under a `data/` folder if using local persistence.

---

## ğŸ“„ License

MIT

---

Built with â¤ï¸ â€” happy researching!
